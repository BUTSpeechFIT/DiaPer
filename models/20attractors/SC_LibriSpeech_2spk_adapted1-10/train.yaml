# training options
activation_loss_BCE_weight: 1.0
activation_loss_DER_weight: 0.0
attractor_existence_loss_weight: 1.0
attractor_frame_comparison: dotprod
condition_frame_encoder: True
context_size: 7
d_latents: 128
detach_attractor_loss: False
dev_batchsize: 128
dropout_attractors: 0.1
dropout_frames: 0.1
feature_dim: 40
frame_encoder_heads: 4
frame_encoder_layers: 4
frame_encoder_units: 2048
frame_shift: 160
frame_size: 400
use_last_samples: True
gpu: 4
gradclip: 5
init_epochs: 90-100
init_model_path: <directory with model trained for 2 speakers>/models
input_transform: logmel_meannorm
intermediate_loss_frameencoder: True
intermediate_loss_perceiver: True
latents2attractors: weighted_average
log_report_batches_num: 1000
max_epochs: 100
model_type: AttractorPerceiver
n_attractors: 20
n_blocks_attractors: 3
n_internal_blocks_attractors: 1
n_latents: 128
n_selfattends_attractors: 2
n_sa_heads_attractors: 4
n_xa_heads_attractors: 4
noam_model_size: 1024
noam_warmup_steps: 100000
norm_loss_per_spk: True
num_frames: 2400
num_speakers: 10
num_workers: 32
optimizer: noam
output_path: <output directory>
pre_xa_heads: 4
sampling_rate: 16000
seed: 3
specaugment: False
subsampling: 10
time_shuffle: False
train_batchsize: 16
train_data_dir: <data_dir>/data/SC_LibriSpeech/v1_1-10spks_all2500h/train/data
use_frame_selfattention: True
use_posenc: False
use_pre_crossattention: True
valid_data_dir: <data_dir>/data/SC_LibriSpeech/v1_1-10spks_all2500h/validation/data
